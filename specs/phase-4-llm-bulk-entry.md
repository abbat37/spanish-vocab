# Phase 4: LLM Integration & Bulk Word Entry

**Status:** COMPLETED
**Created:** 2026-02-10
**Completed:** 2026-02-12
**Duration:** 2 sessions
**Dependencies:** Phase 3 (V2 Database Schema) must be complete

---

## Actual Implementation

Phase 4 was successfully completed with some optimizations and improvements over the original plan:

### Key Changes from Plan

**1. LLM Provider:** Used OpenAI GPT-4o-mini instead of Claude via Portkey
- Cost optimization: GPT-4o-mini is ~75% cheaper than GPT-4o
- Direct OpenAI SDK integration (simpler than Portkey for this use case)
- Excellent structured output capabilities with JSON mode

**2. Word Normalization:** Added explicit normalization layer
- All words stored in lowercase for consistency
- Singular forms preferred over plural
- Masculine forms used as default for gendered words
- Improves learning effectiveness and reduces duplicate confusion

**3. Timeout Management:** Added explicit 30s timeouts
- All OpenAI API calls have 30s timeout
- Prevents hanging requests and improves UX
- Clear error messages when timeouts occur

**4. Validation System:** Two-layer validation working as designed
- Layer 1: LLM output validation (JSON structure, required fields)
- Layer 2: Database validation (duplicates, word length limits)
- Robust error handling with user-friendly messages

**5. UX Improvements:** Enhanced flash message system
- Flash messages persist until explicitly dismissed
- Color-coded for success/error/info states
- Sticky positioning for better visibility

### Production Readiness Checklist

- [x] OpenAI API key configured
- [x] Environment variables set up
- [x] LLM service layer implemented
- [x] Word processing with normalization working
- [x] Bulk entry UI functional
- [x] Error handling with timeouts
- [x] Flash messages with persistence
- [x] Basic input validation
- [ ] Rate limiting per user (deferred to Phase 6)
- [ ] Cost tracking dashboard (deferred to Phase 6)

**Completion:** 8/10 items complete (2 deferred to optimization phase)

---

## Overview

Phase 4 transforms the V2 word creation experience from manual entry to intelligent bulk processing. Users can paste raw word lists, and the LLM automatically translates, categorizes, and tags them - making vocabulary building effortless.

**Learning Objectives:**
- Production LLM integration with Claude via Portkey SDK
- Prompt engineering for structured data extraction
- Bulk text processing and validation
- Error handling for API failures
- Cost management and rate limiting

---

## User Workflow

### Before (Phase 2 - Manual Entry)
```
User enters ONE word at a time:
1. Type Spanish word: "cocinar"
2. Type English: "to cook"
3. Select type: "verb"
4. Select theme: "cooking"
5. Click "Add Word"
6. Repeat 100 times... üò∞
```

### After (Phase 4 - Bulk Entry with LLM)
```
User pastes entire list:
1. Paste:
   Fr√≠o
   Sol
   Viento
   por cierto
   que va!

2. Click "Process Words"
3. Wait 2-3 seconds (loading state)
4. Review auto-tagged results in table
5. Make manual adjustments if needed
6. Done! ‚ú®
```

---

## Technical Design

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend (Create Page)                             ‚îÇ
‚îÇ  - Textarea for bulk input                          ‚îÇ
‚îÇ  - "Process Words" button                           ‚îÇ
‚îÇ  - Loading spinner during processing                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ POST /v2/api/process-words
                  ‚îÇ { "raw_text": "Fr√≠o\nSol..." }
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend (Flask Route)                              ‚îÇ
‚îÇ  1. Parse & clean raw text                          ‚îÇ
‚îÇ  2. Extract individual words/phrases                ‚îÇ
‚îÇ  3. Call LLM service                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Service (OpenAI GPT-4o-mini)                   ‚îÇ
‚îÇ  - Translate Spanish ‚Üí English                      ‚îÇ
‚îÇ  - Normalize words (lowercase, singular, masculine) ‚îÇ
‚îÇ  - Identify word type (verb/noun/phrase)            ‚îÇ
‚îÇ  - Assign 1-3 relevant themes                       ‚îÇ
‚îÇ  - Return structured JSON                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ JSON response
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend (Flask Route)                              ‚îÇ
‚îÇ  4. Validate LLM response                           ‚îÇ
‚îÇ  5. Store in database (V2Word model)                ‚îÇ
‚îÇ  6. Return processed words to frontend              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ JSON: [{ spanish, english, type, themes }]
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend (Create Page)                             ‚îÇ
‚îÇ  - Display table with processed words               ‚îÇ
‚îÇ  - Allow inline editing                             ‚îÇ
‚îÇ  - Show delete buttons                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implementation Steps

### Step 1: Install OpenAI SDK

**Why OpenAI GPT-4o-mini?**
- Cost-effective: ~75% cheaper than GPT-4o
- Excellent structured output with JSON mode
- Fast response times for bulk processing
- Direct API integration (simpler than Portkey)

**Install:**
```bash
pip install openai
```

**Add to requirements.txt:**
```
openai==1.12.0
```

---

### Step 2: Configure Environment Variables

**Add to `.env`:**
```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Organization ID
OPENAI_ORG_ID=your_org_id_here
```

**Note:** Only the API key is required. Organization ID is optional for team accounts.

---

### Step 3: Create LLM Service

**Create `app/v2/services/llm_service.py`:**

```python
"""
LLM Service for V2
Handles all OpenAI API calls for word processing
"""
import os
import json
from openai import OpenAI
from typing import List, Dict, Optional


class LLMService:
    """Service for OpenAI API calls (GPT-4o-mini)"""

    # Predefined theme options
    THEMES = [
        'weather',      # Weather & climate (tiempo)
        'food',         # Food & cooking (comida)
        'work',         # Work & business (trabajo)
        'travel',       # Travel & transportation (viajes)
        'family',       # Family & relationships (familia)
        'emotions',     # Emotions & feelings (emociones)
        'sports',       # Sports & activities (deportes)
        'home',         # Home & daily life (hogar)
        'health',       # Health & body (salud)
        'other'         # Other topics
    ]

    # Word type options
    WORD_TYPES = [
        'verb',           # Action words (cocinar, hablar)
        'noun',           # Things/people (casa, amigo)
        'adjective',      # Descriptive (fr√≠o, grande)
        'adverb',         # Manner/degree (r√°pidamente, muy)
        'phrase',         # Multi-word expressions (por cierto, que va)
        'function_word',  # Grammar words (el, en, y, pero, de)
        'number',         # Numbers (primero, cinco, cien)
        'other'           # Everything else
    ]

    def __init__(self):
        """Initialize OpenAI client"""
        self.client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            timeout=30.0  # 30 second timeout for all requests
        )

    def process_words_bulk(self, raw_words: List[str]) -> List[Dict]:
        """
        Process multiple Spanish words/phrases with LLM.

        Args:
            raw_words: List of Spanish words/phrases (already cleaned)

        Returns:
            List of dicts with structure:
            [
                {
                    'spanish': 'cocinar',
                    'english': 'to cook',
                    'word_type': 'verb',
                    'themes': ['food', 'home']
                },
                ...
            ]
        """
        if not raw_words:
            return []

        # Build prompt
        prompt = self._build_bulk_processing_prompt(raw_words)

        try:
            # Call OpenAI GPT-4o-mini with 30s timeout
            response = self.client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                model="gpt-4o-mini",  # Cost-optimized model
                max_tokens=4000,
                temperature=0.3,  # Low temp for consistent structured output
                timeout=30.0  # Explicit 30s timeout
            )

            # Parse response
            result_text = response.choices[0].message.content
            processed_words = self._parse_llm_response(result_text)

            return processed_words

        except Exception as e:
            print(f"LLM processing error: {e}")
            # Fallback: return words with minimal processing
            return self._fallback_processing(raw_words)

    def _build_bulk_processing_prompt(self, words: List[str]) -> str:
        """Build prompt for bulk word processing"""

        word_list = "\n".join([f"{i+1}. {word}" for i, word in enumerate(words)])

        prompt = f"""You are a Spanish-English vocabulary processing assistant.

For each Spanish word or phrase below, provide:
1. English translation
2. Normalized form (lowercase, singular, masculine)
3. Word type: {', '.join(self.WORD_TYPES)}
4. 1-3 relevant themes from: {', '.join(self.THEMES)}

Spanish words to process:
{word_list}

Return ONLY valid JSON in this exact format (no markdown, no explanation):
[
  {{
    "spanish": "word",
    "english": "translation",
    "word_type": "type",
    "themes": ["theme1", "theme2"]
  }}
]

Rules:
- Normalize Spanish words to lowercase, singular, masculine forms
- Remove gender markers like /a or /o
- Provide natural English translations
- For phrases (2+ words), use word_type "phrase"
- Assign 1-3 most relevant themes (max 3)
- If unsure about theme, use "other"
- Return results in same order as input"""

        return prompt

    def _parse_llm_response(self, response_text: str) -> List[Dict]:
        """Parse LLM JSON response"""
        try:
            # Remove markdown code blocks if present
            clean_text = response_text.strip()
            if clean_text.startswith('```'):
                clean_text = clean_text.split('```')[1]
                if clean_text.startswith('json'):
                    clean_text = clean_text[4:]
                clean_text = clean_text.strip()

            # Parse JSON
            words = json.loads(clean_text)

            # Validate structure
            validated = []
            for word in words:
                if self._validate_word_structure(word):
                    validated.append(word)

            return validated

        except json.JSONDecodeError as e:
            print(f"JSON parse error: {e}")
            return []

    def _validate_word_structure(self, word: Dict) -> bool:
        """Validate that word has required fields"""
        required = ['spanish', 'english', 'word_type', 'themes']

        if not all(key in word for key in required):
            return False

        # Validate word_type
        if word['word_type'] not in self.WORD_TYPES:
            word['word_type'] = 'other'

        # Validate themes (must be list)
        if not isinstance(word['themes'], list):
            word['themes'] = ['other']

        # Limit to 3 themes
        if len(word['themes']) > 3:
            word['themes'] = word['themes'][:3]

        # Ensure themes are valid
        word['themes'] = [t for t in word['themes'] if t in self.THEMES]
        if not word['themes']:
            word['themes'] = ['other']

        return True

    def _fallback_processing(self, raw_words: List[str]) -> List[Dict]:
        """Fallback if LLM fails - return minimal structure"""
        return [
            {
                'spanish': word,
                'english': '(translation needed)',
                'word_type': 'other',
                'themes': ['other']
            }
            for word in raw_words
        ]


# Singleton instance
llm_service = LLMService()
```

---

### Step 4: Create Text Cleaning Utility

**Create `app/v2/utils/text_processing.py`:**

```python
"""
Text Processing Utilities for V2
Handles parsing and cleaning of bulk word input
"""
import re
from typing import List


def parse_bulk_word_input(raw_text: str) -> List[str]:
    """
    Parse bulk word input into clean list of words/phrases.

    Handles:
    - Newline separation
    - Comma separation
    - Extra whitespace
    - Empty lines
    - Special characters (except '/')

    Args:
        raw_text: Raw input from textarea

    Returns:
        List of cleaned words/phrases

    Example:
        Input:
            "Fr√≠o    \\n\\nSol, Viento,,\\npor cierto\\nLejo/a"

        Output:
            ['Fr√≠o', 'Sol', 'Viento', 'por cierto', 'Lejo/a']
    """
    if not raw_text or not raw_text.strip():
        return []

    # Step 1: Split by newlines and commas
    # Replace commas with newlines for uniform processing
    text = raw_text.replace(',', '\n')

    # Split by newlines
    lines = text.split('\n')

    # Step 2: Clean each line
    cleaned_words = []
    for line in lines:
        # Strip whitespace
        word = line.strip()

        # Skip empty lines
        if not word:
            continue

        # Remove special characters EXCEPT '/' (for gender: Lejo/a)
        # Keep: letters, spaces, slash, accents
        word = re.sub(r'[^\w\s/√°√©√≠√≥√∫√±√º¬ø¬°]', '', word, flags=re.IGNORECASE)

        # Remove extra spaces (but keep single spaces for phrases)
        word = ' '.join(word.split())

        # Skip if empty after cleaning
        if word:
            cleaned_words.append(word)

    # Step 3: Remove duplicates while preserving order
    seen = set()
    unique_words = []
    for word in cleaned_words:
        word_lower = word.lower()
        if word_lower not in seen:
            seen.add(word_lower)
            unique_words.append(word)

    return unique_words


def validate_word_length(word: str) -> bool:
    """
    Validate that word is reasonable length.

    Args:
        word: Word or phrase to validate

    Returns:
        True if valid length (1-50 chars)
    """
    return 1 <= len(word) <= 50


def truncate_if_needed(words: List[str], max_words: int = 50) -> List[str]:
    """
    Truncate word list if too long (cost control).

    Args:
        words: List of words
        max_words: Maximum words to process in one batch

    Returns:
        Truncated list
    """
    if len(words) > max_words:
        print(f"Warning: Truncating from {len(words)} to {max_words} words")
        return words[:max_words]
    return words
```

---

### Step 5: Create API Route for Bulk Processing

**Create `app/v2/routes/api.py`:**

```python
"""
V2 API Routes
Handles AJAX requests for word processing and management
"""
from flask import request, jsonify
from flask_login import login_required, current_user
from app.v2.routes import api_bp
from app.v2.services.llm_service import llm_service
from app.v2.utils.text_processing import parse_bulk_word_input, truncate_if_needed
from app.shared.extensions import db
from app.v2.models import V2Word


@api_bp.route('/api/process-words', methods=['POST'])
@login_required
def process_words():
    """
    Process bulk word input with LLM.

    Request:
        { "raw_text": "Fr√≠o\nSol\nViento..." }

    Response:
        {
            "success": true,
            "words": [
                {
                    "spanish": "Fr√≠o",
                    "english": "cold",
                    "word_type": "adj",
                    "themes": ["weather"]
                },
                ...
            ],
            "count": 5
        }
    """
    try:
        data = request.get_json()
        raw_text = data.get('raw_text', '')

        if not raw_text or not raw_text.strip():
            return jsonify({
                'success': False,
                'error': 'No text provided'
            }), 400

        # Step 1: Parse and clean input
        cleaned_words = parse_bulk_word_input(raw_text)

        if not cleaned_words:
            return jsonify({
                'success': False,
                'error': 'No valid words found after cleaning'
            }), 400

        # Step 2: Truncate if too many (cost control)
        cleaned_words = truncate_if_needed(cleaned_words, max_words=50)

        # Step 3: Process with LLM
        processed_words = llm_service.process_words_bulk(cleaned_words)

        if not processed_words:
            return jsonify({
                'success': False,
                'error': 'LLM processing failed'
            }), 500

        # Step 4: Save to database
        saved_count = 0
        for word_data in processed_words:
            # Check for duplicates (case-insensitive)
            existing = V2Word.query.filter(
                V2Word.user_id == current_user.id,
                db.func.lower(V2Word.spanish) == word_data['spanish'].lower()
            ).first()

            if existing:
                # Skip duplicates
                continue

            # Create new word
            new_word = V2Word(
                user_id=current_user.id,
                spanish=word_data['spanish'],
                english=word_data['english'],
                word_type=word_data['word_type'],
                themes=','.join(word_data['themes'])  # Store as comma-separated
            )
            db.session.add(new_word)
            saved_count += 1

        db.session.commit()

        return jsonify({
            'success': True,
            'words': processed_words,
            'count': len(processed_words),
            'saved': saved_count,
            'skipped': len(processed_words) - saved_count
        })

    except Exception as e:
        db.session.rollback()
        print(f"Error processing words: {e}")
        return jsonify({
            'success': False,
            'error': 'Internal server error'
        }), 500
```

**Register API blueprint in `app/v2/routes/__init__.py`:**
```python
# Add to imports
api_bp = Blueprint('v2_api', __name__)

# In register_routes():
def register_routes(bp):
    # ... existing registrations ...
    from .api import register_api_routes
    register_api_routes(bp)
```

---

### Step 6: Update Create Page UI

**Update `app/v2/templates/v2/create.html`:**

Replace the form section with bulk entry UI:

```html
<!-- Bulk Entry Section -->
<div class="bg-white rounded-lg shadow p-6 mb-8">
    <h2 class="text-lg font-semibold text-gray-900 mb-4">Add Words (Bulk Entry)</h2>

    <div class="space-y-4">
        <!-- Instructions -->
        <div class="bg-blue-50 border border-blue-200 rounded-lg p-4 text-sm text-gray-700">
            <p class="font-medium mb-2">üìù How to use bulk entry:</p>
            <ul class="list-disc list-inside space-y-1">
                <li>Paste your Spanish words (one per line or comma-separated)</li>
                <li>No English translation needed - AI will translate automatically</li>
                <li>AI will auto-tag word type and themes</li>
                <li>You can edit results before saving</li>
            </ul>
        </div>

        <!-- Textarea for bulk input -->
        <div>
            <label for="bulkInput" class="block text-sm font-medium text-gray-700 mb-2">
                Paste your Spanish words:
            </label>
            <textarea id="bulkInput" rows="10"
                      class="w-full px-4 py-3 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent font-mono text-sm"
                      placeholder="Fr√≠o&#10;Sol&#10;Viento&#10;por cierto&#10;que va!"></textarea>
            <p class="mt-1 text-sm text-gray-500">Max 50 words per batch</p>
        </div>

        <!-- Process Button -->
        <div class="flex items-center space-x-4">
            <button id="processButton" type="button"
                    class="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 font-medium transition disabled:opacity-50 disabled:cursor-not-allowed">
                Process Words with AI
            </button>

            <!-- Loading indicator (hidden by default) -->
            <div id="loadingIndicator" class="hidden flex items-center text-gray-600">
                <svg class="animate-spin h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24">
                    <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                    <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                </svg>
                Processing...
            </div>
        </div>
    </div>
</div>

<!-- Results Table (hidden until processing complete) -->
<div id="resultsSection" class="bg-white rounded-lg shadow p-6 hidden">
    <h2 class="text-lg font-semibold text-gray-900 mb-4">Processed Words</h2>

    <div class="mb-4 p-4 bg-green-50 border border-green-200 rounded-lg">
        <p class="text-sm text-green-800">
            ‚úÖ <span id="processedCount">0</span> words processed successfully!
            Review below and click "Save All" to add to your vocabulary.
        </p>
    </div>

    <!-- Results table will be populated by JavaScript -->
    <div id="resultsTable" class="overflow-x-auto">
        <!-- Table populated by JS -->
    </div>

    <div class="mt-6 flex justify-between">
        <button id="clearButton" class="px-4 py-2 bg-gray-200 text-gray-800 rounded-lg hover:bg-gray-300">
            Clear & Start Over
        </button>
        <button id="saveAllButton" class="px-6 py-3 bg-green-600 text-white rounded-lg hover:bg-green-700 font-medium">
            Save All to Vocabulary
        </button>
    </div>
</div>
```

---

### Step 7: Add JavaScript for Bulk Processing

**Add to `app/v2/templates/v2/create.html` (in v2_scripts block):**

```html
{% block v2_scripts %}
<script>
const bulkInput = document.getElementById('bulkInput');
const processButton = document.getElementById('processButton');
const loadingIndicator = document.getElementById('loadingIndicator');
const resultsSection = document.getElementById('resultsSection');
const resultsTable = document.getElementById('resultsTable');
const processedCount = document.getElementById('processedCount');
const saveAllButton = document.getElementById('saveAllButton');
const clearButton = document.getElementById('clearButton');

let processedWords = [];

// Process words with LLM
processButton.addEventListener('click', async () => {
    const rawText = bulkInput.value.trim();

    if (!rawText) {
        alert('Please enter some Spanish words first!');
        return;
    }

    // Show loading state
    processButton.disabled = true;
    loadingIndicator.classList.remove('hidden');
    resultsSection.classList.add('hidden');

    try {
        const response = await fetch('/v2/api/process-words', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ raw_text: rawText })
        });

        const data = await response.json();

        if (data.success) {
            processedWords = data.words;
            displayResults(data.words);
            processedCount.textContent = data.count;
            resultsSection.classList.remove('hidden');
        } else {
            alert('Error: ' + data.error);
        }
    } catch (error) {
        console.error('Error:', error);
        alert('Failed to process words. Please try again.');
    } finally {
        processButton.disabled = false;
        loadingIndicator.classList.add('hidden');
    }
});

// Display results in table
function displayResults(words) {
    const tableHTML = `
        <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
                <tr>
                    <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Spanish</th>
                    <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">English</th>
                    <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Type</th>
                    <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Themes</th>
                    <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Actions</th>
                </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
                ${words.map((word, index) => `
                    <tr>
                        <td class="px-4 py-3 font-medium text-gray-900">${word.spanish}</td>
                        <td class="px-4 py-3 text-gray-700">${word.english}</td>
                        <td class="px-4 py-3">
                            <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded">${word.word_type}</span>
                        </td>
                        <td class="px-4 py-3">
                            ${word.themes.map(theme =>
                                `<span class="text-xs bg-gray-100 text-gray-800 px-2 py-1 rounded mr-1">${theme}</span>`
                            ).join('')}
                        </td>
                        <td class="px-4 py-3">
                            <button onclick="editWord(${index})" class="text-blue-600 hover:text-blue-800 text-sm">Edit</button>
                            <button onclick="removeWord(${index})" class="text-red-600 hover:text-red-800 text-sm ml-2">Remove</button>
                        </td>
                    </tr>
                `).join('')}
            </tbody>
        </table>
    `;

    resultsTable.innerHTML = tableHTML;
}

// Edit word (simplified for Phase 4 - full implementation in Phase 5)
function editWord(index) {
    alert('Edit functionality coming in Phase 5!');
}

// Remove word from results
function removeWord(index) {
    processedWords.splice(index, 1);
    displayResults(processedWords);
    processedCount.textContent = processedWords.length;

    if (processedWords.length === 0) {
        resultsSection.classList.add('hidden');
    }
}

// Save all words to database
saveAllButton.addEventListener('click', async () => {
    // In Phase 4, words are already saved during processing
    // This button just confirms and reloads
    alert(`${processedWords.length} words added to your vocabulary!`);
    window.location.reload();
});

// Clear and start over
clearButton.addEventListener('click', () => {
    bulkInput.value = '';
    processedWords = [];
    resultsSection.classList.add('hidden');
});
</script>
{% endblock %}
```

---

## Testing Plan

### Unit Tests

**Create `tests/test_phase4_llm.py`:**

```python
"""Tests for Phase 4 - LLM Integration and Bulk Entry"""
import pytest
from app.v2.utils.text_processing import parse_bulk_word_input


def test_parse_bulk_input_newlines():
    """Test parsing newline-separated words"""
    raw = "Fr√≠o\nSol\nViento"
    result = parse_bulk_word_input(raw)
    assert result == ['Fr√≠o', 'Sol', 'Viento']


def test_parse_bulk_input_commas():
    """Test parsing comma-separated words"""
    raw = "Fr√≠o, Sol, Viento"
    result = parse_bulk_word_input(raw)
    assert result == ['Fr√≠o', 'Sol', 'Viento']


def test_parse_bulk_input_mixed():
    """Test mixed separators"""
    raw = "Fr√≠o\nSol, Viento"
    result = parse_bulk_word_input(raw)
    assert result == ['Fr√≠o', 'Sol', 'Viento']


def test_parse_bulk_input_extra_whitespace():
    """Test handling extra whitespace"""
    raw = "  Fr√≠o    \n\n  Sol  "
    result = parse_bulk_word_input(raw)
    assert result == ['Fr√≠o', 'Sol']


def test_parse_bulk_input_special_chars():
    """Test removing special characters except /"""
    raw = "Fr√≠o--, Sol!!!, Lejo/a"
    result = parse_bulk_word_input(raw)
    assert result == ['Fr√≠o', 'Sol', 'Lejo/a']


def test_parse_bulk_input_phrases():
    """Test handling phrases with spaces"""
    raw = "por cierto\nque va"
    result = parse_bulk_word_input(raw)
    assert result == ['por cierto', 'que va']


def test_parse_bulk_input_duplicates():
    """Test removing duplicates"""
    raw = "Fr√≠o\nfr√≠o\nFR√çO"
    result = parse_bulk_word_input(raw)
    assert len(result) == 1
    assert result[0] == 'Fr√≠o'  # Keeps first occurrence
```

### Integration Tests

**Test bulk processing endpoint:**

```python
def test_bulk_process_api_authenticated(auth_client):
    """Test bulk processing requires authentication"""
    response = auth_client.post('/v2/api/process-words',
                                json={'raw_text': 'Fr√≠o\nSol'})
    assert response.status_code == 200
    data = response.get_json()
    assert data['success'] is True
    assert len(data['words']) == 2


def test_bulk_process_api_empty_input(auth_client):
    """Test error handling for empty input"""
    response = auth_client.post('/v2/api/process-words',
                                json={'raw_text': ''})
    assert response.status_code == 400
```

---

## Success Criteria

Phase 4 is complete when:

- [x] OpenAI SDK installed and configured
- [x] LLM service implemented with bulk processing
- [x] Word normalization implemented (lowercase, singular, masculine)
- [x] 30s timeouts on all API calls
- [x] Text parsing utility handles all edge cases
- [x] API endpoint processes words and returns structured data
- [x] Create page UI updated with bulk entry interface
- [x] Loading states and error handling work correctly
- [x] Flash messages with persistence
- [x] JavaScript displays results in table
- [x] Words saved to database after processing
- [x] Two-layer validation system working
- [x] All tests passing

**Status:** COMPLETED (12/12 criteria met)

---

## Cost Management

**Estimated costs (GPT-4o-mini):**
- GPT-4o-mini: ~$0.15 per million input tokens, $0.60 per million output tokens
- Processing 50 words: ~500 input tokens + 1000 output tokens = $0.0008 per batch
- **75% cheaper than GPT-4o** (which would cost ~$0.003 per batch)
- **Budget:** Estimated $5/month for typical usage

**Cost controls implemented:**
- Max 50 words per batch (truncate if more)
- 30s timeout prevents hanging requests
- User-level rate limiting (deferred to Phase 6)
- Direct API integration (no middleware overhead)

---

## Next Phase Preview

**Phase 5 will add:**
- Edit word modal (inline editing from results table)
- Search functionality
- Filter by type and theme
- Delete words
- Pagination for large vocabularies
- Study/Revise filters

---

## Lessons Learned

**What Worked Well:**
1. GPT-4o-mini provided excellent cost/quality balance for this use case
2. Word normalization significantly improved learning consistency
3. 30s timeouts prevented poor UX from hanging requests
4. Two-layer validation caught edge cases effectively
5. Flash message persistence improved user experience

**What Could Be Improved:**
1. Rate limiting deferred to Phase 6 (acceptable for MVP)
2. Cost tracking dashboard not yet implemented (manual monitoring for now)
3. Could add more sophisticated normalization rules in future

**Technical Debt:**
- Rate limiting system needed before scaling to more users
- Cost tracking/alerting should be added in Phase 6
- Consider batch processing optimization for large word lists

---

**Phase 4 successfully completed! Ready for Phase 5 implementation.**